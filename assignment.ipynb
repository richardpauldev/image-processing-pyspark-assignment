{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Assignment: Building an Image Processing Pipeline with PySpark\n",
    "\n",
    "## Objective\n",
    "This assignment will walk you through a simple PySpark pipeline to classify images from the CIFAR-10 dataset using a machine learning model. Please read the provided code segments and answer the questions, to help understand how the pipeline is constructed. Many of these questions will require outside research into the relevant documentation, found in the resources section.\n",
    "\n",
    "## Setup Instructions\n",
    "These intructions are intended for linux. Start by git cloning the repository, then use the following steps to setup the enviroment: \n",
    "```\n",
    "sudo apt update\n",
    "sudo apt upgrade\n",
    "sudo apt install default-jdk\n",
    "sudo apt install python3 pip3\n",
    "pip3 install pyspark Pillow numpy jupyter\n",
    "```\n",
    "\n",
    "## Resources\n",
    "- [PySpark Documentation](https://spark.apache.org/docs/latest/api/python/index.html)\n",
    "- [Machine Learning with PySpark MLlib](https://spark.apache.org/docs/latest/ml-guide.html)\n",
    "- [CIFAR-10 Dataset](https://www.cs.toronto.edu/~kriz/cifar.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CIFAR-10 Image Processing with PySpark\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"512m\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"100\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Why is it necessary to configure `spark.executor.memory` and `spark.driver.memory`? Include an explanation of what the executor and driver do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What is the purpose of serialization in distributed systems?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR-10 dataset files contain image data and labels in a format that is not immediately suitable for analysis with Spark. You need to transform this data into a format that can be used to create a DataFrame in Spark. Below is the starter function to load a CIFAR-10 batch file into a list of tuples, which will be parallelized into an RDD and then converted into a DataFrame. \n",
    "\n",
    "3. Add line by line comments in the provided code to explain the transformation process, particularly focusing on image reshaping and serialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import io\n",
    "import numpy as np\n",
    "\n",
    "def load_cifar10_batch(file):\n",
    "    \"\"\"\n",
    "    Loads a CIFAR-10 batch file and returns a list of tuples containing image data and labels.\n",
    "    Args:\n",
    "    - file (str): Path to the CIFAR-10 batch file.\n",
    "    Returns:\n",
    "    - list: A list of tuples, where each tuple contains (image_data, label).\n",
    "    \"\"\"\n",
    "    batch = unpickle(file)\n",
    "    data = batch[b'data']\n",
    "    labels = batch[b'labels']\n",
    "    images_and_labels = []\n",
    "\n",
    "    # TODO: Comment the following code\n",
    "    for i in range(len(data)):\n",
    "        image_array = data[i]\n",
    "        image_array_reshaped = image_array.reshape(3, 32, 32).transpose(1, 2, 0)\n",
    "        image = Image.fromarray(image_array_reshaped)\n",
    "        img_byte_arr = io.BytesIO()\n",
    "        image.save(img_byte_arr, format='PNG')\n",
    "        image_bytes = img_byte_arr.getvalue()\n",
    "        images_and_labels.append((image_bytes, labels[i]))\n",
    "    \n",
    "    return images_and_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "DataFrames provide a convenient and efficient way to handle structured data in Spark. You will now take the data loaded from CIFAR-10 files, parallelize it using RDDs (Resilient Distributed Datasets), and then convert these RDDs into DataFrames. This process must handle multiple batches of data to form a comprehensive dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Function to create a DataFrame from a single batch file\n",
    "def create_dataframe_from_batch(file):\n",
    "    images_and_labels = load_cifar10_batch(file)\n",
    "    rdd = spark.sparkContext.parallelize(images_and_labels)\n",
    "    row_rdd = rdd.map(lambda x: Row(image_data=x[0], label=x[1]))\n",
    "    df = spark.createDataFrame(row_rdd)\n",
    "    return df\n",
    "\n",
    "# Load and combine multiple batches\n",
    "df = None\n",
    "for batch_file in batch_files:\n",
    "    batch_df = create_dataframe_from_batch(batch_file)\n",
    "    if df is None:\n",
    "        df = batch_df\n",
    "    else:\n",
    "        df = df.union(batch_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What does the `parallelize` method do, and why is it important in Spark?\n",
    "2. How does the `union` method help in combining data from different sources?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, features need to be numeric and typically normalized. The images in the CIFAR-10 dataset are in byte format and must be converted into a usable form for machine learning models. This task involves writing a UDF that converts the image byte data into a dense vector of normalized pixel values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.types import BinaryType\n",
    "import numpy as np\n",
    "\n",
    "# TODO: INSERT GRAYSCALE UDF HERE\n",
    "def rgb_to_grayscale(image_bytes):\n",
    "    pass\n",
    "\n",
    "def convert_bytes_to_vector(image_bytes):\n",
    "    image = Image.open(io.BytesIO(image_bytes))\n",
    "    array = np.array(image).flatten().astype(float) / 255.0\n",
    "    return Vectors.dense(array)\n",
    "\n",
    "convert_udf = udf(convert_bytes_to_vector, VectorUDT())\n",
    "\n",
    "# TODO apply grayscale UDF here\n",
    "\n",
    "# Apply UDF to the DataFrame\n",
    "df = df.withColumn(\"features\", convert_udf(\"image_data\")) # May need to change the label column, depending on your UDF implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Why is it necessary to normalize the pixel values in image processing?\n",
    "4. What are the benefits of using UDFs in Spark?\n",
    "\n",
    "Color images have three color channels (RGB), while grayscale images combine these channels into a single intensity value that represents different shades of gray. The transformation to grayscale simplifies the data and can be beneficial for various image processing tasks, including reducing the complexity of machine learning models. \n",
    "\n",
    "5. Create a function called rgb_to_grayscale that takes in the image_bytes and returns grayscaled bytes, and a subsequent UDF called grayscale_udf. Hint: Convert to image, use the .convert() method from PIL to grayscale the image, then convert back to bytes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Pipeline\n",
    "\n",
    "Here, we'll set up a machine learning pipeline using PySpark's MLlib. This pipeline will include initializing a RandomForest classifier and fitting it to the training data. This is a critical step in predicting the labels for new data based on learned patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=10)\n",
    "\n",
    "pipeline = Pipeline(stages=[rf])\n",
    "\n",
    "model = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is the role of a RandomForest classifier in this context?\n",
    "2. How does a machine learning pipeline simplify the process of model training and prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(model.transform(test_df))\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Try varying the parameters and models to improve the accuracy. Try varying the number of trees and max depth, or use a different model entirely. Try at least 3 different scenarios and write a brief report on what seems to improve performance. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
