{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Assignment: Building an Image Processing Pipeline with PySpark\n",
    "\n",
    "## Objective\n",
    "Develop a PySpark pipeline to classify images from the CIFAR-10 dataset using a machine learning model.\n",
    "\n",
    "## Prerequisites\n",
    "- Basic knowledge of Python and machine learning.\n",
    "- Access to an environment where PySpark is installed and configured.\n",
    "- CIFAR-10 dataset available in your working directory.\n",
    "\n",
    "## Resources\n",
    "- [PySpark Documentation](https://spark.apache.org/docs/latest/api/python/index.html)\n",
    "- [Machine Learning with PySpark MLlib](https://spark.apache.org/docs/latest/ml-guide.html)\n",
    "- [CIFAR-10 Dataset](https://www.cs.toronto.edu/~kriz/cifar.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CIFAR-10 Image Processing with PySpark\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.memory.fraction\", \"0.6\") \\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"512m\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"100\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Why is it necessary to configure `spark.executor.memory` and `spark.driver.memory`?\n",
    "\n",
    "2. What does setting `spark.memory.fraction` achieve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Why is the purpose of serialization in distributed systems?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR-10 dataset files contain image data and labels in a format that is not immediately suitable for analysis with Spark. You need to transform this data into a format that can be used to create a DataFrame in Spark. Below is the starter function to load a CIFAR-10 batch file into a list of tuples, which will be parallelized into an RDD and then converted into a DataFrame. Add line by line comments in the provided code to explain the transformation process, particularly focusing on image reshaping and serialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import io\n",
    "import numpy as np\n",
    "\n",
    "def load_cifar10_batch(file):\n",
    "    \"\"\"\n",
    "    Loads a CIFAR-10 batch file and returns a list of tuples containing image data and labels.\n",
    "    Args:\n",
    "    - file (str): Path to the CIFAR-10 batch file.\n",
    "    Returns:\n",
    "    - list: A list of tuples, where each tuple contains (image_data, label).\n",
    "    \"\"\"\n",
    "    batch = unpickle(file)\n",
    "    data = batch[b'data']\n",
    "    labels = batch[b'labels']\n",
    "    images_and_labels = []\n",
    "\n",
    "    # TODO: Comment the following code\n",
    "    for i in range(len(data)):\n",
    "        image_array = data[i]\n",
    "        image_array_reshaped = image_array.reshape(3, 32, 32).transpose(1, 2, 0)\n",
    "        image = Image.fromarray(image_array_reshaped)\n",
    "        img_byte_arr = io.BytesIO()\n",
    "        image.save(img_byte_arr, format='PNG')\n",
    "        image_bytes = img_byte_arr.getvalue()\n",
    "        images_and_labels.append((image_bytes, labels[i]))\n",
    "    \n",
    "    return images_and_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "DataFrames provide a convenient and efficient way to handle structured data in Spark. You will now take the data loaded from CIFAR-10 files, parallelize it using RDDs (Resilient Distributed Datasets), and then convert these RDDs into DataFrames. This process must handle multiple batches of data to form a comprehensive dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Function to create a DataFrame from a single batch file\n",
    "def create_dataframe_from_batch(file):\n",
    "    images_and_labels = load_cifar10_batch(file)\n",
    "    rdd = spark.sparkContext.parallelize(images_and_labels)\n",
    "    row_rdd = rdd.map(lambda x: Row(image_data=x[0], label=x[1]))\n",
    "    df = spark.createDataFrame(row_rdd)\n",
    "    return df\n",
    "\n",
    "# Load and combine multiple batches\n",
    "df = None\n",
    "for batch_file in batch_files:\n",
    "    batch_df = create_dataframe_from_batch(batch_file)\n",
    "    if df is None:\n",
    "        df = batch_df\n",
    "    else:\n",
    "        df = df.union(batch_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. What does the `parallelize` method do, and why is it important in Spark?\n",
    "5. How does the `union` method help in combining data from different sources?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, features need to be numeric and typically normalized. The images in the CIFAR-10 dataset are in byte format and must be converted into a usable form for machine learning models. This task involves writing a UDF that converts the image byte data into a dense vector of normalized pixel values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "import numpy as np\n",
    "\n",
    "def convert_bytes_to_vector(image_bytes):\n",
    "    image = Image.open(io.BytesIO(image_bytes))\n",
    "    array = np.array(image).flatten().astype(float) / 255.0\n",
    "    return Vectors.dense(array)\n",
    "\n",
    "convert_udf = udf(convert_bytes_to_vector, VectorUDT())\n",
    "\n",
    "# Apply UDF to the DataFrame\n",
    "df = df.withColumn(\"features\", convert_udf(\"image_data\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Why is it necessary to normalize the pixel values in image processing?\n",
    "7. What are the benefits of using UDFs in Spark?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
