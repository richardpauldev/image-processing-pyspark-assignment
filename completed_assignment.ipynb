{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Assignment: Building an Image Processing Pipeline with PySpark\n",
    "\n",
    "## Objective\n",
    "This assignment will walk you through a simple PySpark pipeline to classify images from the CIFAR-10 dataset using a machine learning model. Please read the provided code segment and answer the questions, to help understand how the pipeline is constructed. \n",
    "\n",
    "## Setup Instructions\n",
    "These intructions are intended for linux. Start by git cloning the repository, then use the following steps to setup the enviroment: \n",
    "```\n",
    "sudo apt update\n",
    "sudo apt upgrade\n",
    "sudo apt install default-jdk\n",
    "sudo apt install python3 pip3\n",
    "pip3 install pyspark Pillow numpy jupyter\n",
    "```\n",
    "\n",
    "## Resources\n",
    "- [PySpark Documentation](https://spark.apache.org/docs/latest/api/python/index.html)\n",
    "- [Machine Learning with PySpark MLlib](https://spark.apache.org/docs/latest/ml-guide.html)\n",
    "- [CIFAR-10 Dataset](https://www.cs.toronto.edu/~kriz/cifar.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CIFAR-10 Image Processing with PySpark\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"512m\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"100\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Why is it necessary to configure `spark.executor.memory` and `spark.driver.memory`? Include an explanation of what the executor and driver do.\n",
    "\n",
    "`spark.executor.memory` speicifies the amount of memory to be used by each executor process in Spark. Executors are responsible for executing tasks and returning restulst to the driver. By configuring this setting, you control the memory resources available for processing data on each node, which directly affects performance and efficiency. If the memory is too low, tasks may fail due to memory overflow; if too high, you might underutilize resources.\n",
    "\n",
    "`spark.driver.memory` sets the amount of memory to use for the Spark driver, which orchestrates the operations of Spark applications. The driver holds the information about the Spark application and is responsible for scheduling jobs, negotiating with the cluster manager, and managing the overall execution process. Sufficient memory allocation here is crucial for managing application workflows effectively, especially when collecting data back to the driver for final processing, which can be memory-intensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. What is the purpose of serialization in distributed systems?\n",
    "\n",
    "Serialization in distributed systems involves converting data structures or object states into a format that can be easily stored or transmitted and then reconstructed later. The primary purposes of serialization are:\n",
    "\n",
    "- Data Exchange: Allows data to be efficiently transferred over the network between different components of a distributed system (like between nodes of a Spark cluster or from clients to servers).\n",
    "\n",
    "- Persistence: Enables data to be written to or read from storage mediums in a form that can be readily consumed by applications, ensuring that the state can be preserved beyond the lifetime of the process.\n",
    "\n",
    "- Performance: Serialized data is typically compacted, reducing I/O overhead and improving the performance of data-intensive operations across the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR-10 dataset files contain image data and labels in a format that is not immediately suitable for analysis with Spark. You need to transform this data into a format that can be used to create a DataFrame in Spark. Below is the starter function to load a CIFAR-10 batch file into a list of tuples, which will be parallelized into an RDD and then converted into a DataFrame. \n",
    "\n",
    "4. Add line by line comments in the provided code to explain the transformation process, particularly focusing on image reshaping and serialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import io\n",
    "import numpy as np\n",
    "\n",
    "def load_cifar10_batch(file):\n",
    "    \"\"\"\n",
    "    Loads a CIFAR-10 batch file and returns a list of tuples containing image data and labels.\n",
    "    Args:\n",
    "    - file (str): Path to the CIFAR-10 batch file.\n",
    "    Returns:\n",
    "    - list: A list of tuples, where each tuple contains (image_data, label).\n",
    "    \"\"\"\n",
    "    batch = unpickle(file)\n",
    "    data = batch[b'data']\n",
    "    labels = batch[b'labels']\n",
    "    images_and_labels = []\n",
    "\n",
    "    # TODO: Comment the following code\n",
    "    for i in range(len(data)): # Iterate over each image's data in the batch\n",
    "        image_array = data[i] # Access the raw image data for the i-th image.\n",
    "        image_array_reshaped = image_array.reshape(3, 32, 32).transpose(1, 2, 0) # Reshapes the flat array into a 3D array (3 channels, 32x32 pixels) and transposes the axes for a proper image format.\n",
    "        image = Image.fromarray(image_array_reshaped) # Converts the NumPy array into a PIL Image object.\n",
    "\n",
    "        img_byte_arr = io.BytesIO() # Creates an in-memory byte stream to store the image data.\n",
    "        image.save(img_byte_arr, format='PNG') # Saves the Image object as a PNG to the byte stream.\n",
    "        image_bytes = img_byte_arr.getvalue() # Extracts the byte value of the image data frome the stream.\n",
    "\n",
    "        images_and_labels.append((image_bytes, labels[i])) # Appends a tuple of image bytes and the corresponding label to the list.\n",
    "    \n",
    "    return images_and_labels # Returns the list of tuples with image data and labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "DataFrames provide a convenient and efficient way to handle structured data in Spark. You will now take the data loaded from CIFAR-10 files, parallelize it using RDDs (Resilient Distributed Datasets), and then convert these RDDs into DataFrames. This process must handle multiple batches of data to form a comprehensive dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Function to create a DataFrame from a single batch file\n",
    "def create_dataframe_from_batch(file):\n",
    "    images_and_labels = load_cifar10_batch(file)\n",
    "    rdd = spark.sparkContext.parallelize(images_and_labels)\n",
    "    row_rdd = rdd.map(lambda x: Row(image_data=x[0], label=x[1]))\n",
    "    df = spark.createDataFrame(row_rdd)\n",
    "    return df\n",
    "\n",
    "# Load and combine multiple batches\n",
    "df = None\n",
    "for batch_file in batch_files:\n",
    "    batch_df = create_dataframe_from_batch(batch_file)\n",
    "    if df is None:\n",
    "        df = batch_df\n",
    "    else:\n",
    "        df = df.union(batch_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What does the `parallelize` method do, and why is it important in Spark?\n",
    "\n",
    "The parallelize method in Spark is used to distribute a local Python collection (like lists or arrays) across the nodes of a Spark cluster. This method creates a Resilient Distributed Dataset (RDD), which is Spark's fundamental data structure. RDDs are distributed across many nodes so that the data can be processed in parallel.\n",
    "\n",
    "- Scalability: By parallelizing data, Spark can handle computations over large datasets that exceed the memory of a single machine. It allows the framework to scale horizontally, distributing tasks across many servers.\n",
    "- Fault Tolerance: RDDs are resilient because they track the lineage of transformations applied to each dataset. If any part of the dataset is lost due to node failure, Spark can recompute just the lost partitions from lineage, minimizing the computation needed to recover the lost data.\n",
    "- Performance: Parallel processing is central to Spark’s ability to handle big data. Tasks that would take a long time to execute on a single machine can be divided into smaller tasks and executed simultaneously across multiple machines, greatly reducing processing times.\n",
    "\n",
    "2. How does the `union` method help in combining data from different sources?\n",
    "\n",
    "The union method in Spark is used to combine two RDDs or DataFrames, appending one dataset to another. The resulting dataset will include all rows from both datasets. For union to work correctly, both datasets must have the same schema and number of columns; however, the columns do not necessarily need to be of the same type.\n",
    "\n",
    "- Data Integration: union is essential for combining datasets from different sources, such as different data files, databases, or even data streams. It allows for easy and efficient data aggregation, essential in data analysis and machine learning scenarios where data comes from multiple sources.\n",
    "- Simplicity and Efficiency: Using union, data engineers can merge datasets in a straightforward manner without needing complex join operations. This can be particularly efficient when dealing with large volumes of data that simply need to be processed as a single dataset.\n",
    "- Flexibility: It provides the ability to perform more complex data transformations and analyses by enabling the integration of diverse datasets, thereby enriching the data context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, features need to be numeric and typically normalized. The images in the CIFAR-10 dataset are in byte format and must be converted into a usable form for machine learning models. This task involves writing a UDF that converts the image byte data into a dense vector of normalized pixel values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.types import BinaryType\n",
    "import numpy as np\n",
    "\n",
    "# TODO: INSERT GRAYSCALE UDF HERE\n",
    "def rgb_to_grayscale(image_bytes):\n",
    "    image = Image.open(io.BytesIO(image_bytes))\n",
    "    grayscale_image = image.convert('L')\n",
    "    img_byte_arr = io.BytesIO()\n",
    "    grayscale_image.save(img_byte_arr, format='PNG')\n",
    "    return img_byte_arr.getvalue()\n",
    "\n",
    "def convert_bytes_to_vector(image_bytes):\n",
    "    image = Image.open(io.BytesIO(image_bytes))\n",
    "    array = np.array(image).flatten().astype(float) / 255.0 # Flatten and normalize Pixel Values\n",
    "    return Vectors.dense(array)\n",
    "\n",
    "grayscale_udf = udf(rgb_to_grayscale, BinaryType())\n",
    "convert_udf = udf(convert_bytes_to_vector, VectorUDT())\n",
    "\n",
    "# TODO apply grayscale UDF here\n",
    "df = df.withColumn(\"grayscale_data\", grayscale_udf(\"image_data\"))\n",
    "\n",
    "# Apply UDF to the DataFrame\n",
    "df = df.withColumn(\"features\", convert_udf(\"grayscale_data\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Why is it necessary to normalize the pixel values in image processing?\n",
    "\n",
    "Normalizing pixel values in image processing to a uniform scale, typically between 0 and 1, is crucial for maintaining consistency and enhancing machine learning models, as it ensures uniformity across varying image sources and optimizes training processes by improving convergence and model accuracy. In addition, normalized data reduces computational complexity, easing memory demands and speeding up calculations. \n",
    "\n",
    "4. What are UDFs? What are the benefits of using UDFs in Spark?\n",
    "\n",
    "The use of User-Defined Functions (UDFs) in Spark offers significant benefits, including the ability to extend Spark SQL's capabilities with custom functionality tailored to specific processing requirements. UDFs can be seamlessly integrated into Spark SQL, allowing complex procedural logic to be embedded within SQL queries, leveraging Spark’s distributed processing power. This integration not only enhances performance by allowing parallel processing across cluster nodes but also promotes reusability and consistency across different Spark applications and queries, effectively customizing and expanding the utility of Spark in big data environments.\n",
    "\n",
    "Color images have three color channels (RGB), while grayscale images combine these channels into a single intensity value that represents different shades of gray. The transformation to grayscale simplifies the data and can be beneficial for various image processing tasks, including reducing the complexity of machine learning models. \n",
    "\n",
    "5. Create a function called rgb_to_grayscale that takes in the image_bytes and returns grayscaled bytes, and a subsequent UDF called grayscale_udf. Hint: Convert to image, use the .convert() method from PIL to grayscale the image, then convert back to bytes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Pipeline\n",
    "\n",
    "Here, we'll set up a machine learning pipeline using PySpark's MLlib. This pipeline will include initializing a RandomForest classifier and fitting it to the training data. This is a critical step in predicting the labels for new data based on learned patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=10)\n",
    "\n",
    "pipeline = Pipeline(stages=[rf])\n",
    "\n",
    "model = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is the role of a RandomForest classifier in this context?\n",
    "\n",
    "In this situation, the RandomForest classifier is used for the machine learning part of the pipeline, in fact it is the entire pipeline. This is how pyspark will handle feature extraction, and fit the classifier to the given training data. \n",
    "\n",
    "2. How does a machine learning pipeline simplify the process of model training and prediction?\n",
    "\n",
    "A machine learning pipeline in PySpark simplifies the process of model training and prediction by encapsulating all the steps involved in data preprocessing, model training, and predictions into a single, manageable workflow. This structured approach ensures that all steps are performed in a specific sequence, maintaining consistency across different runs and datasets. It automates repetitive tasks, reduces the likelihood of errors, and enhances the reproducibility of results. By using a pipeline, data scientists can easily experiment with different models and preprocessing techniques without rewriting substantial amounts of code, making the process of model development both efficient and scalable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(model.transform(test_df))\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Try varying the parameters and models to improve the accuracy. Try varying the number of trees and max depth, or use a different model entirely. Try at least 3 different scenarios and write a brief report on what seems to improve performance. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
